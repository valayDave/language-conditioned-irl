from typing import List
import cma
import shutil
from .trainer import Simulator,\
    VREP_SCENE,\
    DEFAULT_LOGGER_PROJECT_NAME,\
    HEADLESS,\
    dir_exists
import json
import neptune
import numpy as np 
import cv2
from .reward_model import RoboRewardFnMixin
from .agent import RobotAgent,CMAESAgent
from .utils import Voice
from ..mountaincar.gym import RewardNormalizer
import os 

DEFAULT_EXPERIMENT_NAME = 'CMAES-Experiments'

# todo : this will have a parameterized policy created from the function
def run_simulation_episode(simulator_args:dict,):
    pass


def collect_evaluations(trajectories:List,episode_dict:dict,simulator_args:dict,reward_fn:RoboRewardFnMixin):
    """
    Serially runs some episodes using the trajectories generated by CMAES. 
    """
    simulator = Simulator(**simulator_args)
    evaluation_results = []
    for trajectory in trajectories:
        reward_fn_dict,episode_perf_stats,reached_goal,text_input = simulator.simulate_trajectory(episode_dict,trajectory)
        reward_value = reward_fn.get_rewards(text_input,reward_fn_dict)
        print("Reward for Trajectory : ",reward_value.squeeze(0).numpy()[0])
        evaluation_results.append(
            (
                reward_fn_dict,
                episode_perf_stats,
                reached_goal,
                text_input,
                reward_value.squeeze(0).numpy()[0]
            )
        )
    simulator.shutdown()
    return evaluation_results

class CMAESSimulation:
    """
    Evoluationary method for policy search 

    https://lilianweng.github.io/lil-log/2019/09/05/evolution-strategies.html#applications-es-in-deep-reinforcement-learning
    popsize : controls the size of the population to run. 
    """
    def __init__(self,
                 args=None,
                 scenepath=VREP_SCENE,
                 project_name=DEFAULT_LOGGER_PROJECT_NAME,
                 experiment_name=DEFAULT_EXPERIMENT_NAME,
                 api_token=None,
                 video_save_dir='./video'):
        self.simulator_args = dict(
            args=args,
            scenepath=scenepath,
            headless=True
        )
        self.api_token=api_token
        self.video_save_dir=video_save_dir
        self.project_name=project_name
        self.experiment_name=experiment_name
        self._setup_video_dir()
        self.voice = Voice(load=False)
    
    def _setup_video_dir(self):
        if dir_exists(self.video_save_dir):
            shutil.rmtree(self.video_save_dir)
        os.mkdir(self.video_save_dir)

    def picking_task_cmaes(self,
                           chosen_file: str,
                           reward_fn: RoboRewardFnMixin,
                            num_gaussians:int=50,
                            max_iter:int=2000,
                            population_size:int=100):
        with open(chosen_file, "r") as fh:
            data = json.load(fh)
        self.run_pick_cmaes_episodes(data,\
                                    reward_fn,\
                                    max_iter=max_iter,\
                                    num_gaussians=num_gaussians,\
                                    population_size=population_size)

    def get_core_config(self,reward_fn:RoboRewardFnMixin,text_context:str=None):
        ENV = "Picking Task"
        reward_ob = dict(
            run_name=reward_fn.experiment_name,
            model_pth=reward_fn.loaded_checkpoint,
        )
        config = dict(
            env=ENV,
            text_context=text_context,
            **reward_ob
        )
        return config
     
    def _make_logger(self,reward_fn:RoboRewardFnMixin,text_context:str,train:bool=False):
        config = self.get_core_config(reward_fn,text_context=text_context)
        # $ Create Logger. 
        if self.api_token is not None:
            neptune.init(self.project_name,api_token=self.api_token)
        else:
            neptune.init(self.project_name,backend=neptune.OfflineBackend())

        exp_name = f'{self.experiment_name}'
        if not train:
            exp_name = f'{exp_name}-test'
        neptune.create_experiment(name=exp_name, params=config)
    
    def _parse_language_input(self, voice, phs):
        def _quantity(voice):
            res = 0
            for word in self.voice.synonyms["little"]:
                if voice.find(word) >= 0:
                    res = 1
            for word in self.voice.synonyms["much"]:
                if voice.find(word) >= 0:
                    res = 2
            return res

        def _difficulty(voice):
            if phs == 2:
                voice = " ".join(voice.split()[4:])
            shapes = self.voice.synonyms["round"] + \
                self.voice.synonyms["square"]
            colors = self.voice.synonyms["small"] + \
                self.voice.synonyms["large"]
            sizes = self.voice.synonyms["red"] + self.voice.synonyms["green"] + \
                self.voice.synonyms["blue"] + \
                self.voice.synonyms["yellow"] + self.voice.synonyms["pink"]

            shapes_used = 0
            for word in shapes:
                if voice.find(word) >= 0:
                    shapes_used = 1
            colors_used = 0
            for word in colors:
                if voice.find(word) >= 0:
                    colors_used = 1
            sizes_used = 0
            for word in sizes:
                if voice.find(word) >= 0:
                    sizes_used = 1
            return shapes_used + colors_used + sizes_used

        data = {}
        data["original"] = voice
        data["features"] = _difficulty(voice)
        data["quantity"] = _quantity(voice)
        return data
    
    @staticmethod
    def save_generation_video(generation:int,solution_id:int,video_array:np.array,freq=20,video_save_dir='./cmaes_simulations'):
        width,height= video_array[0].shape[0], video_array[0].shape[1]
        path = os.path.join(video_save_dir,f'generation-{generation}-{solution_id}.avi')
        fourcc = cv2.VideoWriter_fourcc(*'DIVX')
        writer = cv2.VideoWriter(path, fourcc, 20, (640,480))
        for i in range(0,len(video_array),freq):
            x = video_array[i].astype('uint8')
            img= cv2.resize(x,(640,480))
            writer.write(img)
        writer.release()


    def _log_values(self,evaluated_trajectories,generation_number:int):
        print(f"Saving Values For Generation : {generation_number}")
        rewards = []
        for idx,trajtup in enumerate(evaluated_trajectories):
            reward_fn_dict,\
            episode_perf_stats,\
            reached_goal,\
            text_input,\
            reward = trajtup
            self.save_generation_video(generation_number,idx,reward_fn_dict['image_sequence'],video_save_dir=self.video_save_dir)
            neptune.log_metric(f'is_success-gen-{generation_number}', idx, y=reached_goal)
            neptune.log_metric(f'reward-gen-{generation_number}', idx, y=reward)
            neptune.log_metric(f'distance_to_obj-gen-{generation_number}', idx,
                                y=episode_perf_stats['distance'])
            rewards.append(reward)
        return rewards
    
    
    def run_pick_cmaes_episodes(self,
                                data,
                                reward_fn: RoboRewardFnMixin,
                                num_gaussians:int=50,
                                max_iter:int=2000,
                                population_size:int=100):
        try:
            text_input = data["voice"]
            # $ make logger to run episode
            self._make_logger(reward_fn, text_input, train=True)
            # $ Set size of traj based on RW fn Constraints
            max_trajectory_size = reward_fn.max_traj_length
            # $ Get Metadata about text
            input_text_meta = self._parse_language_input(text_input, 1)

            final_position = Simulator.get_object_target_position(
                data['ints'], data['floats'], data['target/id']
            )
            # Agent state : Joint positions + gripper state
            es = cma.CMAEvolutionStrategy((7 *num_gaussians)*[0], 0.5,inopts=dict(popsize=population_size,maxiter=max_iter))
            agent = CMAESAgent(num_gaussians)
            generation_number = 0
            while not es.stop():
                solutions = es.ask()
                print("Running Generation Simulation",len(solutions))
                trajectories = [ agent.get_robot_trajectory(max_trajectory_size,solution) for solution in solutions]
                evaluated_trajectories = collect_evaluations(trajectories,data,self.simulator_args,reward_fn)
                rewards = self._log_values(evaluated_trajectories,generation_number)
                es.tell(solutions, rewards)
                es.logger.add()  # write data to disc to be plotted
                generation_number+=1
                es.disp()

        except KeyboardInterrupt as e:
            print("Keyboard Interrupt Occured")
            self.shutdown()
            neptune.stop()
        finally:
            neptune.log_artifact(self.video_save_dir)
